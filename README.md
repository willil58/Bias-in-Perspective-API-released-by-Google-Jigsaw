I wanted to take a look at the Perspective API released by Google Jigsaw and examine it for potential biases. After looking at the scores, I decided to see how the scores would differ if I compared some positive and negative words to their abbreviations. My hypothesis was that the scores for the abbreviated forms of both the positive and negative scores would be lower compared to the fully spelled out words or phrases. 

To test this, I created four lists of words and phrases and ran them through the algorithm.  I picked the words for my list based off of the initial analysis I did of the training set. I then used those words to help me come up with my lists or words and phrases

The first two lists were comprised of positive and words and phrases. They both had the same words but one contained the abbreviated forms. The other two lists contained negative words and phrases. Again, they both had the same words but one contained the abbreviated forms.

I ran all four through the algorithm and was able to get scores for all four lists. When I looked at the scores of the full words and compared them to the abbreviated words, I found that the negative abbreviations received lower scores than the fully written words. This is what I hypothesized would happen. However, when I looked at the positive words and phrases the abbreviated forms received higher scores. This was surprising and went against my hypothesis. I concluded that there is a bias in the algorithm when it comes to scoring abbreviations. It’s biased towards negative words and rates them lower than the same words fully spelled out. It’s also biased towards positive abbreviations and rates them higher than the same words fully spelled out. The theory I have for the discrepancy on the scores is due to where the training data is from. The developers used comments from sites like Wikipedia and The New York Times. The people who use these sites are more likely to comment with the full version of a word or phrase and not use abbreviations. So, the training set didn’t include abbreviations as much as other sites like Twitter or Instagram where it’s more common to use abbreviations. Thus, the algorithm isn’t as familiar with abbreviations and mistakes their meaning. 

One additional issue I’d like to point out was there were several very common abbreviations I wanted to use that the algorithm couldn’t analyze. It marked them as being in an unsupported language. I theorize this is likely an issue because new abbreviations are being created and trending all the time so it’s hard to include them all in the test data.
